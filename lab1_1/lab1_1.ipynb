{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"5878267e","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1681748401083,"user_tz":-180,"elapsed":5864,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"}},"outputId":"a9c06e9b-e351-4cf8-ea02-959f45551002"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"]}],"source":["# Please do not change this cell because some hidden tests might depend on it.\n","import os\n","\n","# Otter grader does not handle ! commands well, so we define and use our\n","# own function to execute shell commands.\n","def shell(commands, warn=True):\n","    \"\"\"Executes the string `commands` as a sequence of shell commands.\n","     \n","       Prints the result to stdout and returns the exit status. \n","       Provides a printed warning on non-zero exit status unless `warn` \n","       flag is unset.\n","    \"\"\"\n","    file = os.popen(commands)\n","    print (file.read().rstrip('\\n'))\n","    exit_status = file.close()\n","    if warn and exit_status != None:\n","        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n","    return exit_status\n","\n","shell(\"\"\"\n","ls requirements.txt >/dev/null 2>&1\n","if [ ! $? = 0 ]; then\n"," rm -rf .tmp\n"," git clone https://github.com/cs236299-2023-spring/lab1-1.git .tmp\n"," mv .tmp/tests ./\n"," mv .tmp/requirements.txt ./\n"," rm -rf .tmp\n","fi\n","pip install -q -r requirements.txt\n","\"\"\")"],"id":"5878267e"},{"cell_type":"code","execution_count":3,"metadata":{"id":"cf6948a5","executionInfo":{"status":"ok","timestamp":1681748401548,"user_tz":-180,"elapsed":469,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"}}},"outputs":[],"source":["# Initialize Otter\n","import otter\n","grader = otter.Notebook()"],"id":"cf6948a5"},{"cell_type":"markdown","metadata":{"id":"382f17fa"},"source":["# Course 236299\n","## Lab 1-1 – Types, tokens, and representing text"],"id":"382f17fa"},{"cell_type":"code","execution_count":4,"metadata":{"id":"682e20f5","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1681748407281,"user_tz":-180,"elapsed":5735,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"}},"outputId":"92e25ebe-d09f-48b0-e77f-fa5c148c99a3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["import math\n","import re\n","import sys\n","\n","import torch\n","import nltk\n","\n","nltk.download('punkt', quiet=True) # this module is used to tokenize the text"],"id":"682e20f5"},{"cell_type":"markdown","metadata":{"id":"e3af2f9c"},"source":["Where we're headed: Nearest neighbor text classification works by classifying a novel text with the same class as that of the training text that is closest according to some distance metric. These metrics are calculated based on representations of the texts. In this lab, we'll introduce some different representations and you'll use nearest neighbor classification to predict the speaker of sentences selected from a children's book.\n","    \n","The objectives of this lab are to:\n","\n","* Clarify terminology around words and texts,\n","* Manipulate different representations of words and texts,\n","* Apply these representations to calculate text similarity, and\n","* Classify documents by a simple nearest neighbor model.\n","   \n","In this and later labs, we will have you carry out several exercises in notebook cells. The cells you are to do are marked '`#TODO`'. They will typically have a `...` where your code or answer should go. Where specified, you needn't write code to calculate the answer, but instead, simply work out the answer yourself and enter it."],"id":"e3af2f9c"},{"cell_type":"markdown","metadata":{"id":"c118a10c"},"source":["New bits of Python used for the first time in the _solution set_ for this lab, and which you may therefore find useful:\n","\n","* `math.acos`\n","* `math.pi`\n","* `re.match`\n","* `set`\n","* `sorted`\n","* `str.join`\n","* `str.lower`\n","* `torch.dot`\n","* `torch.linalg.norm`\n","* `torch.maximum`\n","* `torch.minimum`\n","* `torch.stack`\n","* `torch.sum`\n","* `torch.Tensor.type`\n","* `torch.where`\n","* `torch.zeros`\n","* `torch.zeros_like`\n","* `nltk.tokenize.word_tokenizer`\n","* `nltk.tokenize.WhitespaceTokenize`"],"id":"c118a10c"},{"cell_type":"markdown","metadata":{"id":"1284bb12"},"source":["# Counting words\n","\n","<img src=\"https://github.com/nlp-course/data/blob/master/Seuss/seuss%20-%201966%20-%20green%20eggs%20and%20ham.gif?raw=true\" width=150 align=right />\n","\n","Here are five sentences from Dr. Seuss's [_Green Eggs and Ham_](https://en.wikipedia.org/wiki/Green_Eggs_and_Ham):\n","\n","    Would you like them here or there?\n","    I would not like them here or there.\n","    I would not like them anywhere.\n","    I do not like green eggs and ham.\n","    I do not like them, Sam-I-am.\n","\n","We'll make this text available in the variable `text`."],"id":"1284bb12"},{"cell_type":"code","execution_count":5,"metadata":{"id":"17e256f4","executionInfo":{"status":"ok","timestamp":1681748407282,"user_tz":-180,"elapsed":66,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"}}},"outputs":[],"source":["text = \"\"\"\n","    Would you like them here or there?\n","    I would not like them here or there.\n","    I would not like them anywhere.\n","    I do not like green eggs and ham.\n","    I do not like them, Sam-I-am.\n","    \"\"\""],"id":"17e256f4"},{"cell_type":"markdown","metadata":{"id":"ef8f670e"},"source":["A Python string like this is, of course, a sequence of characters. But we think of this text as a sequence of sentences each composed of a sequence of words. How many words are there in this text? That is a fraught question, for several reasons, including\n","\n","* The type-token distinction\n","* Tokenization issues\n","* Normalization\n","\n","## Types versus tokens\n","\n","In determining the number of words in `text`, are we talking about word _types_ or word _tokens_. (For instance, there are five _tokens_ of the word _type_ 'like'.)\n","\n","How many word tokens are there in total in this text? (Just count them manually.) Assign the number to the variable `token_count` in the next cell.\n","<!--\n","BEGIN QUESTION\n","name: token_count\n","-->"],"id":"ef8f670e"},{"cell_type":"code","execution_count":6,"metadata":{"id":"b420ccfc","executionInfo":{"status":"ok","timestamp":1681748407282,"user_tz":-180,"elapsed":65,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"}}},"outputs":[],"source":["#TODO - define `token_count` to be the number of tokens in `text`\n","token_count = 35"],"id":"b420ccfc"},{"cell_type":"code","execution_count":7,"metadata":{"id":"9ffb6cc5","colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"status":"ok","timestamp":1681748407283,"user_tz":-180,"elapsed":66,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"}},"outputId":"4e2ad99b-2a55-433c-85e5-02065c61ad56"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":7}],"source":["grader.check(\"token_count\")"],"id":"9ffb6cc5"},{"cell_type":"markdown","metadata":{"id":"a1299a50"},"source":["How many word types are there? (Again, you can just count manually.)\n","<!--\n","BEGIN QUESTION\n","name: type_count\n","-->"],"id":"a1299a50"},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":64,"status":"ok","timestamp":1681748407283,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"18027f78"},"outputs":[],"source":["#TODO - define `type_count` to be the number of types in `text`\n","type_count = 16"],"id":"18027f78"},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"elapsed":64,"status":"ok","timestamp":1681748407283,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"8c1837a4","outputId":"24a2793a-a539-425c-bd13-fc94f9e4fb90"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":9}],"source":["grader.check(\"type_count\")"],"id":"8c1837a4"},{"cell_type":"markdown","metadata":{"id":"ef170119"},"source":["<!-- BEGIN QUESTION -->\n","\n","**Question:** The set of types of a language is referred to as its _vocabulary_. Are there more types or tokens as you calculated above? Could it be otherwise?\n","<!--\n","BEGIN QUESTION\n","name: type_vs_token_count\n","manual: true\n","-->"],"id":"ef170119"},{"cell_type":"markdown","metadata":{"id":"718b9e4e"},"source":["**Answer:** In our calculations, we made some decisions about the types or token we counted:\n","\n","\n","\n","(1) Punctuation – such symbols can be counted as tokens for some tasks, but we have not counted them (we have ignored these symbols).\n","\n","\n","\n","(2) Capitalization – the word 'would' appears both in the capitalized form ('Would') and in the regular form ('would'), and we have decided to account for these as two different types.\n","\n","\n","\n","(3) Multiple-word idioms – the idiom 'Sam-I-am' refers semantically to a single entity, but it can be treated literally as three words, and we have decided to count it as a single token."],"id":"718b9e4e"},{"cell_type":"markdown","metadata":{"id":"cf0e9941"},"source":["<!-- END QUESTION -->\n","\n","\n","\n","## Tokenization "],"id":"cf0e9941"},{"cell_type":"markdown","metadata":{"id":"271da622"},"source":["Did you count 'there?' as one token or two? This raises the issue of _tokenization_ of text, how to decide where the token boundaries occur. For instance, here's a simple way to split a string – to _tokenize_ it – in Python by splitting at whitespace."],"id":"271da622"},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":62,"status":"ok","timestamp":1681748407284,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"2fb18584"},"outputs":[],"source":["def whitespace_tokenize(str):\n","    return str.split()"],"id":"2fb18584"},{"cell_type":"markdown","metadata":{"id":"5ba2c5bc"},"source":["Try it out on the `text` defined above.\n","<!--\n","BEGIN QUESTION\n","name: tokens_whitespace\n","-->"],"id":"5ba2c5bc"},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":61,"status":"ok","timestamp":1681748407284,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"cf11c9b3"},"outputs":[],"source":["#TODO - define `tokens` to be the tokens as defined by the `whitespace_tokenize` function\n","tokens = whitespace_tokenize(text)"],"id":"cf11c9b3"},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"elapsed":60,"status":"ok","timestamp":1681748407284,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"7ef2be11","outputId":"24944e8a-daa2-4d81-ead8-717dce04a3f5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":12}],"source":["grader.check(\"tokens_whitespace\")"],"id":"7ef2be11"},{"cell_type":"markdown","metadata":{"id":"9a566d23"},"source":["Using this tokenization method, count the number of tokens in the text, this time using Python to do the work.\n","<!--\n","BEGIN QUESTION\n","name: token_count_whitespace\n","-->"],"id":"9a566d23"},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":58,"status":"ok","timestamp":1681748407285,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"493803ab"},"outputs":[],"source":["#TODO - place your token count here\n","token_count_2 = len(tokens)"],"id":"493803ab"},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"elapsed":56,"status":"ok","timestamp":1681748407285,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"710a37ef","outputId":"b0712876-591e-4d2d-aed7-bcb886d81655"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":14}],"source":["grader.check(\"token_count_whitespace\")"],"id":"710a37ef"},{"cell_type":"markdown","metadata":{"id":"e3a59370"},"source":["Arguably, we _should_ split off punctuation as separate tokens, but even there, some care must be taken. We don't want to split 'don't' into three tokens or 'Sam-I-Am' into five. (There's a good argument to be made however that the string 'don't' should be construed as two tokens, namely, 'do' and 'n't', but that's beyond the scope of today's discussion.)\n","\n","Here, we provide an alternative tokenizer that splits tokens at whitespace and splits off punctuation at the beginning and end of non-whitespace regions as separate tokens as well. It makes use of [the Python `re` module](https://docs.python.org/3/library/re.html) for regular expressions to specify the splitting process. Look over the code and make sure you understand what's going on. You might find [this online tool](https://regexr.com/) useful."],"id":"e3a59370"},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":56,"status":"ok","timestamp":1681748407286,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"efbd32aa"},"outputs":[],"source":["def punc_tokenize(str):\n","    return [tok for tok in re.split('(\\W*?)\\s+', str) if tok != '']\n"],"id":"efbd32aa"},{"cell_type":"markdown","metadata":{"id":"db167e71"},"source":["Now how many tokens are there in the text if tokenized in this way?\n","<!--\n","BEGIN QUESTION\n","name: token_count_punc\n","-->"],"id":"db167e71"},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":55,"status":"ok","timestamp":1681748407286,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"4e0ca711"},"outputs":[],"source":["#TODO\n","token_count_3 = len(punc_tokenize(text))"],"id":"4e0ca711"},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"elapsed":56,"status":"ok","timestamp":1681748407287,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"9c74a353","outputId":"f6580b2a-5ad3-4238-ae37-5fdef3544376"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":17}],"source":["grader.check(\"token_count_punc\")"],"id":"9c74a353"},{"cell_type":"markdown","metadata":{"id":"4345a81a"},"source":["## Normalization\n","\n","This tokenization method counts 'Would' and 'would' (capitalized and uncapitalized) as separate types. Is that a good idea? This raises the issue of text _normalization_.\n","\n","Define a function `normalize_token` that normalizes tokens by making them lowercase if at most the first character is uppercase. (Hints [here](https://docs.python.org/3/library/stdtypes.html#str.lower) and [here](https://docs.python.org/3/library/re.html#re.match). These are also listed in the hint cell at the top of the lab, so we'll mostly stop providing these hints from here on.)\n","<!--\n","BEGIN QUESTION\n","name: normalize_token\n","-->"],"id":"4345a81a"},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":54,"status":"ok","timestamp":1681748407287,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"8389fc0e"},"outputs":[],"source":["#TODO - implement normalize_token, which returns the normalized word for a single word `str`\n","def normalize_token(str):\n","    pattern = r\"^[A-Za-z][^A-Z]*$\"\n","    if re.match(pattern, str):\n","        return str.lower()\n","    return str"],"id":"8389fc0e"},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"elapsed":54,"status":"ok","timestamp":1681748407288,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"b105d0b6","outputId":"4ad5eeee-2226-47f8-ed18-9e58addaf599"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":19}],"source":["grader.check(\"normalize_token\")"],"id":"b105d0b6"},{"cell_type":"markdown","metadata":{"id":"3fbf6b9e"},"source":["Now define `norm_tokens_punc` to be the sequence of normalized tokens as tokenized by `punc_tokenize`\n","<!--\n","BEGIN QUESTION\n","name: norm_tokens_punc\n","-->"],"id":"3fbf6b9e"},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":52,"status":"ok","timestamp":1681748407288,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"3f3c87a4"},"outputs":[],"source":["#TODO\n","norm_tokens_punc = [normalize_token(token) for token in punc_tokenize(text)]"],"id":"3f3c87a4"},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"elapsed":54,"status":"ok","timestamp":1681748407290,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"d12dcaf8","outputId":"2724e372-c08c-4c52-c496-97b006c97b30"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":21}],"source":["grader.check(\"norm_tokens_punc\")"],"id":"d12dcaf8"},{"cell_type":"markdown","metadata":{"id":"2b30684e"},"source":["How many types are there when tokenized and normalized in this way?\n","<!--\n","BEGIN QUESTION\n","name: type_count_norm_punc\n","-->"],"id":"2b30684e"},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":53,"status":"ok","timestamp":1681748407290,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"23483fc1"},"outputs":[],"source":["#TODO\n","type_count_norm_punc = len(norm_tokens_punc)"],"id":"23483fc1"},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"elapsed":52,"status":"ok","timestamp":1681748407290,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"919fac6e","outputId":"4e89b315-ffd3-4f23-d9e7-9daf8b2a8598"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":23}],"source":["grader.check(\"type_count_norm_punc\")"],"id":"919fac6e"},{"cell_type":"markdown","metadata":{"id":"fa9efef6"},"source":["## Using prebuilt tokenizers\n","\n","Tokenization is so commonly needed that many packages provide pre-built tokenizers of various sorts.\n","We'll use one from [Natural Language Tool Kit (NLTK)](http://nltk.org). It's already been imported above under the name `nltk`."],"id":"fa9efef6"},{"cell_type":"markdown","metadata":{"id":"c89fc85d"},"source":["Define two tokenizers, versions of `whitespace_tokenize` and a normalized version of `punc_tokenize` above, using [the `nltk.tokenize.WhitespaceTokenizer`](https://www.nltk.org/api/nltk.tokenize.regexp.html#nltk.tokenize.regexp.WhitespaceTokenizer) and [`nltk.tokenize.word_tokenize`](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize) respectively.\n","Note that `nltk.tokenize.word_tokenize` only tokenizes the string, so we normalize the string by lowering the string's characters.\n","<!--\n","BEGIN QUESTION\n","name: nltk_whitespace_tokenize_and_nltk_normpunc_tokenize\n","-->"],"id":"c89fc85d"},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":52,"status":"ok","timestamp":1681748407291,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"95a21260"},"outputs":[],"source":["#TODO\n","def nltk_whitespace_tokenize(str):\n","    return nltk.tokenize.WhitespaceTokenizer().tokenize(str)\n","    \n","def nltk_normpunc_tokenize(str):\n","    str = str.lower()\n","    return nltk.tokenize.word_tokenize(str)"],"id":"95a21260"},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"elapsed":53,"status":"ok","timestamp":1681748407292,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"ec3bd25a","outputId":"2b9ebcd6-ea85-42d1-a009-af079cee30a7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":25}],"source":["grader.check(\"nltk_whitespace_tokenize_and_nltk_normpunc_tokenize\")"],"id":"ec3bd25a"},{"cell_type":"markdown","metadata":{"id":"1b0c0dd7"},"source":["Now define `nltk_norm_tokens_punc` to be the sequence of normalized tokens as tokenized by the pre-built tokenizer `nltk_normpunc_tokenize`\n","<!--\n","BEGIN QUESTION\n","name: nltk_norm_tokens_punc\n","-->"],"id":"1b0c0dd7"},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":53,"status":"ok","timestamp":1681748407293,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"8de6f546"},"outputs":[],"source":["#TODO\n","nltk_norm_tokens_punc = nltk_normpunc_tokenize(text)"],"id":"8de6f546"},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"elapsed":53,"status":"ok","timestamp":1681748407294,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"23769f63","outputId":"daa0639a-60d0-454b-ed69-8302b439a455"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":27}],"source":["grader.check(\"nltk_norm_tokens_punc\")"],"id":"23769f63"},{"cell_type":"markdown","metadata":{"id":"5c6ebacd"},"source":["Now we should be able to print out the last few tokens of the sample text, tokenized using these functions."],"id":"5c6ebacd"},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":185,"status":"ok","timestamp":1681748408145,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"44c50841","outputId":"27ccd360-a8ac-43a4-9c70-574efcb596ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["['green', 'eggs', 'and', 'ham.', 'I', 'do', 'not', 'like', 'them,', 'Sam-I-am.']\n","['ham', '.', 'i', 'do', 'not', 'like', 'them', ',', 'sam-i-am', '.']\n"]}],"source":["print(nltk_whitespace_tokenize(text)[-10:])\n","print(nltk_normpunc_tokenize(text)[-10:])"],"id":"44c50841"},{"cell_type":"markdown","metadata":{"id":"4b40075a"},"source":["> _Meta-comment:_ Because it's important that you get practice both with implementing the ideas in the course from first principles and also with using prebuilt software that provides similar functionality, we'll often have you engage in this seemingly redundant process of first implementing a small example and then applying a prebuilt method to do much the same thing. The effort may be duplicative, but it is not wasted."],"id":"4b40075a"},{"cell_type":"markdown","metadata":{"id":"ffd08746"},"source":["# Representing words\n","\n","In this section, we'll explore some simple representations for tokens, as a step on the way to representing texts – sentences or documents:\n","\n","## String encoding\n","We've already seen string encoding above, representing a token of a word type by a string specific to that type: a token 'green' represented by an instance of the Python string `'green'`, for instance, or 'Sam-I-am' represented by `'Sam-I-am'`. So let's move on.\n","\n","## 1-hot encoding\n","Given a vocabulary for a language, we can associate each type with an integer, say by its index in a vector. We've already imported the `torch` module; we'll use `torch` tensors for the index vector. For the Seuss text, we can use this list to represent the ordered vocabulary:"],"id":"ffd08746"},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":182,"status":"ok","timestamp":1681748408147,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"7f0b676a","outputId":"665f52fd-178f-4b26-d8b6-503eaf504424"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[',',\n"," '.',\n"," '?',\n"," 'and',\n"," 'anywhere',\n"," 'do',\n"," 'eggs',\n"," 'green',\n"," 'ham',\n"," 'here',\n"," 'i',\n"," 'like',\n"," 'not',\n"," 'or',\n"," 'sam-i-am',\n"," 'them',\n"," 'there',\n"," 'would',\n"," 'you']"]},"metadata":{},"execution_count":29}],"source":["vocabulary = sorted(set(nltk_norm_tokens_punc))\n","vocabulary"],"id":"7f0b676a"},{"cell_type":"markdown","metadata":{"id":"535dbbdd"},"source":["### A digression on `torch` tensors\n","\n","Recall that `torch` tensors allow for vectorized computations: many operations on them work [componentwise](https://en.wikipedia.org/wiki/Pointwise#Componentwise_operations), that is, separately for each component of the tensor, rather than on the tensor all at once. Compare the following two operations, first on lists, then on `torch` tensors."],"id":"535dbbdd"},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":176,"status":"ok","timestamp":1681748408148,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"b04d1ad2","outputId":"297b6b9d-dbc7-4165-905d-d8d2db712b5b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":30}],"source":["[1, 2] == 1"],"id":"b04d1ad2"},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":171,"status":"ok","timestamp":1681748408150,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"ea81a165","outputId":"c7b648ab-b8fd-42a2-db06-2ab72c72b881"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ True, False])"]},"metadata":{},"execution_count":31}],"source":["torch.tensor([1, 2]) == 1"],"id":"ea81a165"},{"cell_type":"markdown","metadata":{"id":"e766e6b8"},"source":["This behavior of tensors is quite powerful, allowing for simply specifying complex operations and for efficient, even parallelizable, computation of them. You'll want ot take advantage of these characteristics of tensors where possible, here and in future assignments.\n","\n","But back to the 1-hot representation."],"id":"e766e6b8"},{"cell_type":"markdown","metadata":{"id":"41a9d162"},"source":["In the _1-hot representation_ of words, a token is then represented by a bit vector (again given as a `torch` tensor), with a 1 at the index of the token's type. (For consistency with some later `torch` functions, we'll take the elements to be floats rather than ints. The `.type` method is useful for converting the type, and it even conveniently broadcasts over the tensors.) For instance, the 1-hot representation of the comma token ',' would be"],"id":"41a9d162"},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":165,"status":"ok","timestamp":1681748408151,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"360d6a82","outputId":"37a142d5-c4ee-49eb-9ca1-6981970a1a8c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0.])"]},"metadata":{},"execution_count":32}],"source":["torch.tensor([1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]).type(torch.float32)"],"id":"360d6a82"},{"cell_type":"markdown","metadata":{"id":"5314b43a"},"source":["Conversion back and forth between these various representations is useful. Define functions `str_to_onehot` and `onehot_to_str` that convert between the string and one-hot representations using a vocabulary array to define the  conversion. \n","\n","Ideally, in your implementation, you'll want to take advantage of the componentwise nature of many tensor operations discussed above."],"id":"5314b43a"},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":155,"status":"ok","timestamp":1681748408152,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"6c7dfd88"},"outputs":[],"source":["#TODO\n","def str_to_onehot(wordtype, vocab):\n","    \"\"\"Returns the 1-hot representation of `wordtype` with vocabulary \n","    `vocab`.\n","    The returned value should be a torch.tensor with data type float.\n","    \"\"\"\n","    l = [1 if wordtype == vocab[i] else 0 for i in range(len(vocab))]\n","    return torch.tensor(l).type(torch.float32)\n","    \n","\n","def onehot_to_str(onehot, vocab):\n","    \"\"\"Returns the string representation of `onehot`, a one-hot \n","    representation of a word type, with vocabulary `vocab`.\n","    \"\"\"\n","    return vocab[list(onehot == 1).index(True)]"],"id":"6c7dfd88"},{"cell_type":"markdown","metadata":{"id":"939cb540"},"source":["Now use `str_to_onehot` to define the variable `anywhere_1hot` to be the 1-hot representation for a token of the type 'anywhere'. \n","<!--\n","BEGIN QUESTION\n","name: anywhere_1hot\n","-->"],"id":"939cb540"},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":154,"status":"ok","timestamp":1681748408153,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"434f67a1"},"outputs":[],"source":["#TODO\n","anywhere_1hot = str_to_onehot(\"anywhere\",vocabulary)"],"id":"434f67a1"},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"elapsed":154,"status":"ok","timestamp":1681748408154,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"1cd13ffb","outputId":"3f50fef5-381b-4755-f89d-f93b3e97c075"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":35}],"source":["grader.check(\"anywhere_1hot\")"],"id":"1cd13ffb"},{"cell_type":"markdown","metadata":{"id":"011e3db1"},"source":["You can verify that the conversion worked correctly by inverting it using `onehot_to_str`, which we've done in the following unit test.\n","<!--\n","BEGIN QUESTION\n","name: anywhere_1hot_reverse\n","-->"],"id":"011e3db1"},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"elapsed":149,"status":"ok","timestamp":1681748408155,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"623fda8b","outputId":"08167a42-7f3a-44ab-e729-1d9cf3bb4360"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":36}],"source":["grader.check(\"anywhere_1hot_reverse\")"],"id":"623fda8b"},{"cell_type":"markdown","metadata":{"id":"a72e9715"},"source":["# Representing texts\n","\n","## The set-of-words representation\n","\n","We can represent a whole text (a sequence of words) by manipulating the vector representations of the words within the text. For instance, we can take the componentwise maximum of the vectors. We refer to this as the _set-of-words_ representation.\n","\n","Here we've defined a function `set_of_words` that returns the set of words representation for a token sequence."],"id":"a72e9715"},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":149,"status":"ok","timestamp":1681748408157,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"c5b4d849"},"outputs":[],"source":["def set_of_words(tokens, vocabulary):\n","    \"\"\"Returns the set-of-words representation as a tensor of floats for the \n","    sequence of `tokens` using the `vocabulary` to specify the conversion.\n","    \"\"\"\n","    onehots = torch.stack([str_to_onehot(token, vocabulary) for token in tokens])\n","    return torch.amax(onehots, 0).type(torch.float32)"],"id":"c5b4d849"},{"cell_type":"markdown","metadata":{"id":"e55abd17"},"source":["This representation for a text is a vector that has a `1` for each word type that occurs in the text. The vector represents the [characteristic function](https://en.wikipedia.org/wiki/Characteristic_function) for the subset of vocabulary words that appear in the text; hence the term 'set of words'.\n"],"id":"e55abd17"},{"cell_type":"markdown","metadata":{"id":"a58a8fa4"},"source":["What is the set-of-words representation for the example text 'I would not, would not, here or there.'?\n","<!--\n","BEGIN QUESTION\n","name: example_sow\n","-->"],"id":"a58a8fa4"},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":149,"status":"ok","timestamp":1681748408158,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"aae3d445"},"outputs":[],"source":["#TODO - define the variable to be the set of words representation for the example text\n","# 'I would not, would not, here or there.'\n","# Use `nltk_normpunc_tokenize` tokenizer\n","example_sow = set_of_words(nltk_normpunc_tokenize('I would not, would not, here or there.'),vocabulary)"],"id":"aae3d445"},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"elapsed":149,"status":"ok","timestamp":1681748408159,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"8a12a44c","outputId":"1dc45f25-559b-40c6-e812-7977d6a50034"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":39}],"source":["grader.check(\"example_sow\")"],"id":"8a12a44c"},{"cell_type":"markdown","metadata":{"id":"16495c0a"},"source":["## The bag of words representation\n","\n","If instead, we take the componentwise _addition_ of the vectors instead of the maximum, the text representation provides the _frequency_ of each word type in the text. We refer to this representation as the _[bag](https://en.wikipedia.org/wiki/Multiset) of words_ representation. \n","\n","Define a function `bag_of_words`, analogous to `set_of_words` above, that returns the bag-of-words representation for a token sequence."],"id":"16495c0a"},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":148,"status":"ok","timestamp":1681748408160,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"ddb042a9"},"outputs":[],"source":["#TODO\n","def bag_of_words(tokens, vocabulary):\n","    onehots = torch.stack([str_to_onehot(token, vocabulary) for token in tokens])\n","    return torch.sum(onehots, 0).type(torch.float32)"],"id":"ddb042a9"},{"cell_type":"markdown","metadata":{"id":"6ce9a236"},"source":["What is the bag of words representation for the example text 'I would not, would not, here or there.'?\n","<!--\n","BEGIN QUESTION\n","name: example_bow\n","-->"],"id":"6ce9a236"},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":149,"status":"ok","timestamp":1681748408162,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"97c69a91"},"outputs":[],"source":["#TODO - define the variable to be the bag of words representation for the example text\n","# 'I would not, would not, here or there.'\n","# Use the `nltk_normpunc_tokenize` tokenizer\n","example_bow = bag_of_words(nltk_normpunc_tokenize('I would not, would not, here or there.'),vocabulary)"],"id":"97c69a91"},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"elapsed":149,"status":"ok","timestamp":1681748408163,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"a28bee58","outputId":"683b1e43-d137-4a27-9d29-70da68dc867b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":42}],"source":["grader.check(\"example_bow\")"],"id":"a28bee58"},{"cell_type":"markdown","metadata":{"id":"9e297489"},"source":["# Document similarity metrics\n","\n","Consider the following text classification problem: Each sentence in _Green Eggs and Ham_ is spoken by one of two characters, Sam-I-Am and Guy-Am-I. We want to be able to classify new sentences as (most likely) being uttered by one of the two.\n","\n","A simple method for text classification is the _nearest neighbor_ method. We select the class for the new sentence that is the same as the class of the \"nearest\" (most similar) sentence for which we already know the class. (You'll experiment much more with this text classification method in the next lab.)\n","\n","To perform nearest neighbor classification, we need a method for measuring the (metaphorical) distance between two texts based on their representations. We'll explore a few methods here:\n","\n","* Hamming distance\n","* Jaccard distance\n","* Euclidean distance\n","* cosine distance\n","\n","You'll implement code for all of these distance metrics. Try to implement the functions using `torch` tensor functions only, without explicit iteration over the elements in the vector.\n","\n","We'll take a look at the distances among the following sentences:\n","\n","1. Would you like them here or there?\n","2. I would not like them here or there.\n","3. Do you like green eggs and ham?\n","4. I do not like them Sam-I-Am.\n","\n","We'll start with the set of words representations of these sentences:"],"id":"9e297489"},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":148,"status":"ok","timestamp":1681748408164,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"ea7ef901"},"outputs":[],"source":["examples = \"\"\"Would you like them here or there?\n","              I would not like them here or there.\n","              Do you like green eggs and ham?\n","              I do not like them Sam-I-Am.\"\"\" \\\n","           .split(\"\\n\")\n","sows = [set_of_words(nltk_normpunc_tokenize(sentence), vocabulary) \n","            for sentence in examples]"],"id":"ea7ef901"},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":149,"status":"ok","timestamp":1681748408165,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"1836d353","outputId":"996d449b-c36d-4a50-ad43-7684297dd88e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n","         1.]),\n"," tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n","         0.]),\n"," tensor([0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n","         1.]),\n"," tensor([0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n","         0.])]"]},"metadata":{},"execution_count":44}],"source":["sows"],"id":"1836d353"},{"cell_type":"markdown","metadata":{"id":"62950a96"},"source":["## Hamming distance\n","\n","The Hamming distance between two vectors is the number of positions at which they differ. Define a function `hamming_distance` that computes the Hamming distance between two vectors.\n","<!--\n","BEGIN QUESTION\n","name: hamming_distance\n","-->"],"id":"62950a96"},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":141,"status":"ok","timestamp":1681748408166,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"9b03e409"},"outputs":[],"source":["#TODO - implement hamming_distance. The returned value should be an integer.\n","def hamming_distance(v1, v2):\n","    return (v1 != v2).view(torch.int8).sum()"],"id":"9b03e409"},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"elapsed":141,"status":"ok","timestamp":1681748408167,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"e5209a16","outputId":"d19ccf35-ab4e-41f9-b37d-87c74672f24e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":46}],"source":["grader.check(\"hamming_distance\")"],"id":"e5209a16"},{"cell_type":"markdown","metadata":{"id":"b237323b"},"source":["Now we can generate the Hamming distances among all of the sample sentences in a little table. Do the values make sense?"],"id":"b237323b"},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":139,"status":"ok","timestamp":1681748408167,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"06b70da4","outputId":"f277d35a-d19e-43cf-c1b9-f585cd0e70c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["   0    5   10   11 \n","   5    0   15    6 \n","  10   15    0   11 \n","  11    6   11    0 \n"]}],"source":["for i in range(4):\n","    for j in range(4):\n","        print(f\"{hamming_distance(sows[i], sows[j]):4} \", end='')\n","    print()"],"id":"06b70da4"},{"cell_type":"markdown","metadata":{"id":"a59e2847"},"source":["## Jaccard distance\n","\n","The Jaccard distance between two sets (and remember that these bit strings basically represent sets) is one minus the number of elements in their intersection divided by the number of elements in their union.\n","\n","$$ D_{jaccard}(v_1, v_2) = 1 - \\frac{| v_1 \\cap v_2 |}{| v_1 \\cup v_2 |} $$\n","\n","Define a function `jaccard_distance` to compute the Jaccard distance between two set-of-words representations.\n","<!--\n","BEGIN QUESTION\n","name: jaccard_distance\n","-->"],"id":"a59e2847"},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":132,"status":"ok","timestamp":1681748408168,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"fe97a03d"},"outputs":[],"source":["#TODO\n","def jaccard_distance(v1, v2):\n","    return 1 - torch.bitwise_and(v1.type(torch.int8),v2.type(torch.int8)).sum() / torch.bitwise_or(v1.type(torch.int8),v2.type(torch.int8)).sum()"],"id":"fe97a03d"},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"elapsed":132,"status":"ok","timestamp":1681748408169,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"2795d7e2","outputId":"0b101f2b-e20e-404a-de62-6e6d3482bef0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":49}],"source":["grader.check(\"jaccard_distance\")"],"id":"2795d7e2"},{"cell_type":"markdown","metadata":{"id":"0348e6e7"},"source":["Again, here's a table of the Jaccard distances among the sample sentences."],"id":"0348e6e7"},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":131,"status":"ok","timestamp":1681748408170,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"0d7ff497","outputId":"d086b219-566c-4c5f-b8e8-dc6cfc204e19"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.000 0.455 0.769 0.846 \n","0.455 0.000 0.938 0.545 \n","0.769 0.938 0.000 0.846 \n","0.846 0.545 0.846 0.000 \n"]}],"source":["for i in range(4):\n","    for j in range(4):\n","        print(f\"{jaccard_distance(sows[i], sows[j]):5.3f} \", end='')\n","    print()"],"id":"0d7ff497"},{"cell_type":"markdown","metadata":{"id":"86854a8c"},"source":["## Euclidean distance\n","\n","The Euclidean distance between two vectors is the norm of the vector between them, that is,\n","\n","$$ D_{euclidean}(\\mathbf{x}, \\mathbf{y}) = |\\mathbf{x} - \\mathbf{y}| $$\n","\n","where $|\\mathbf{z}|$, the norm of a vector $\\mathbf{z}$, is calculated as\n","\n","$$ |\\mathbf{z}| = \\sqrt{\\sum_{i=1}^N \\mathbf{z}_i^2} $$\n","\n","Fortunately, `torch` provides the function [`torch.linalg.norm`](https://pytorch.org/docs/stable/generated/torch.linalg.norm.html#torch.linalg.norm) to compute the norm, and the vector between two vectors can be computed by componentwise subtraction.\n","\n","Define a function `euclidean_distance` to compute the Euclidean distance between two vectors.\n","<!--\n","BEGIN QUESTION\n","name: euclidean_distance\n","-->"],"id":"86854a8c"},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":120,"status":"ok","timestamp":1681748408171,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"bc7ea99e"},"outputs":[],"source":["#TODO\n","def euclidean_distance(v1, v2):\n","    return torch.linalg.norm(v1 - v2)"],"id":"bc7ea99e"},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"elapsed":121,"status":"ok","timestamp":1681748408173,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"3dea5b02","outputId":"84b4a89a-ca8a-4dcd-9162-25f6f03e55c5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":52}],"source":["grader.check(\"euclidean_distance\")"],"id":"3dea5b02"},{"cell_type":"markdown","metadata":{"id":"c7e6d704"},"source":["Again, here's a table of the Euclidean distances among the sample sentences."],"id":"c7e6d704"},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":119,"status":"ok","timestamp":1681748408174,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"7ac20734","outputId":"e42c8e75-e218-4420-a71c-fcbdde209a2f"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.000 2.236 3.162 3.317 \n","2.236 0.000 3.873 2.449 \n","3.162 3.873 0.000 3.317 \n","3.317 2.449 3.317 0.000 \n"]}],"source":["for i in range(4):\n","    for j in range(4):\n","        print(f\"{euclidean_distance(sows[i], sows[j]):5.3f} \", end='')\n","    print()"],"id":"7ac20734"},{"cell_type":"markdown","metadata":{"id":"f8c5ace6"},"source":["## Cosine distance\n","\n","The _cosine similarity_ of two vectors of length $N$ is the cosine of the angle that they form, which is computed as the dot product of the two vectors divided by their norms.\n","\n","$$ cos(\\mathbf{x}, \\mathbf{y}) = \n","      \\frac{\\sum_{i=1}^N \\mathbf{x}_i \\cdot \\mathbf{y}_i}{|\\mathbf{x}| \\cdot |\\mathbf{y}|} $$\n","\n","This isn't a distance metric, but a similarity metric. For vectors of non-negative numbers, it ranges from 0 to 1, where 0 is maximally different and 1 is maximally similar. To turn it into a distance metric, then, we take the inverse cosine (to convert the cosine to an angle between $\\pi$ and 0) and divide by $\\pi$.\n","\n","$$ D_{cosine}(\\mathbf{x}, \\mathbf{y}) = \\frac{cos^{-1}(cos(\\mathbf{x}, \\mathbf{y}))}{\\pi} $$\n","\n","Since we're using `torch`, some of these functions are already provided. See hints [here](https://pytorch.org/docs/stable/generated/torch.dot.html), [here](https://pytorch.org/docs/stable/generated/torch.linalg.norm.html), and [here](https://docs.python.org/3/library/math.html#math.acos).\n","\n","(To avoid some math domain errors, we recommend that you use the function `safe_acos` that we've provided to compute the inverse cosine function instead of using `math.acos` directly.)"],"id":"f8c5ace6"},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":112,"status":"ok","timestamp":1681748408175,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"9618dc72"},"outputs":[],"source":["def safe_acos(x):\n","    \"\"\"Returns the arc cosine of `x`. Unlike `math.acos`, it \n","       does not raise an exception for values of `x` out of range, \n","       but rather clips `x` at -1..1, thereby avoiding math domain\n","       errors in the case of numerical errors.\"\"\"\n","    return math.acos(math.copysign(min(1.0, abs(x)), x))"],"id":"9618dc72"},{"cell_type":"code","execution_count":55,"metadata":{"executionInfo":{"elapsed":111,"status":"ok","timestamp":1681748408176,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"22ded3f7"},"outputs":[],"source":["#TODO\n","def cosine_distance(v1, v2):\n","    \"\"\"Returns the cosine distance between two vectors\"\"\"\n","    return safe_acos( v1@v2 / (torch.linalg.norm(v1) * torch.linalg.norm(v2)))/math.pi"],"id":"22ded3f7"},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":46},"executionInfo":{"elapsed":111,"status":"ok","timestamp":1681748408177,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"b75201fd","outputId":"e3b50f39-b585-428e-f106-48f977b3f999"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":56}],"source":["grader.check(\"cosine_distance\")"],"id":"b75201fd"},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":110,"status":"ok","timestamp":1681748408178,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"d88a3333","outputId":"a51479ed-ac25-487b-c2ce-e82e8259271f"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.000 0.250 0.378 0.414 \n","0.250 0.000 0.462 0.283 \n","0.378 0.462 0.000 0.414 \n","0.414 0.283 0.414 0.000 \n"]}],"source":["for i in range(4):\n","    for j in range(4):\n","        print(f\"{cosine_distance(sows[i], sows[j]):5.3f} \", end='')\n","    print()"],"id":"d88a3333"},{"cell_type":"markdown","metadata":{"id":"c6cf44d8"},"source":["In the next lab, you'll use some of these distance metrics to automatically classify text using nearest neighbor classification."],"id":"c6cf44d8"},{"cell_type":"markdown","metadata":{"id":"a14894e2"},"source":["<!-- BEGIN QUESTION -->\n","\n","# Lab debrief\n","\n","**Question:** We're interested in any thoughts you have about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following: \n","\n","* Was the lab too long or too short?\n","* Were the readings appropriate for the lab? \n","* Was it clear (at least after you completed the lab) what the points of the exercises were? \n","* Are there additions or changes you think would make the lab better?\n","\n","<!--\n","BEGIN QUESTION\n","name: open_response_debrief\n","manual: true\n","-->"],"id":"a14894e2"},{"cell_type":"markdown","metadata":{"id":"7a51c5ff"},"source":["**Answer:**  \n","  * We have found the lab to be too long and we had to continue it after class.  \n","  * Yes, they were very comprehensive.  \n","  * Yes, it was clear but we found it too detailed.\n","  * It would be nice to see applications or hints of the future topics in the course."],"id":"7a51c5ff"},{"cell_type":"markdown","metadata":{"id":"eea4fe3b"},"source":["<!-- END QUESTION -->\n","\n","\n","\n","# Submission Instructions\n","\n","This lab should be submitted to Gradescope. Submit both the code [here](https://www.gradescope.com/courses/522028?submit_assignment_id=2745476) and the pdf [here](https://www.gradescope.com/courses/522028?submit_assignment_id=1927894), or by logging in to the course page on Gradescope. \n","\n","Make sure that you have passed all public tests by running `grader.check_all()` below before submitting. Note that there are hidden tests on Gradescope, the results of which will be revealed after the submission deadline."],"id":"eea4fe3b"},{"cell_type":"markdown","metadata":{"id":"751f7be4"},"source":["# End of lab 1-1"],"id":"751f7be4"},{"cell_type":"markdown","metadata":{"id":"78a8f5d7"},"source":["---\n","\n","To double-check your work, the cell below will rerun all of the autograder tests."],"id":"78a8f5d7"},{"cell_type":"code","execution_count":58,"metadata":{"executionInfo":{"elapsed":104,"status":"ok","timestamp":1681748408179,"user":{"displayName":"Yorai Levi","userId":"01332226094735695949"},"user_tz":-180},"id":"1ea49682","colab":{"base_uri":"https://localhost:8080/","height":864},"outputId":"cb49f252-d0c2-4379-c709-91372eae03ab"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["anywhere_1hot:\n","\n","    All tests passed!\n","    \n","\n","anywhere_1hot_reverse:\n","\n","    All tests passed!\n","    \n","\n","cosine_distance:\n","\n","    All tests passed!\n","    \n","\n","euclidean_distance:\n","\n","    All tests passed!\n","    \n","\n","example_bow:\n","\n","    All tests passed!\n","    \n","\n","example_sow:\n","\n","    All tests passed!\n","    \n","\n","hamming_distance:\n","\n","    All tests passed!\n","    \n","\n","jaccard_distance:\n","\n","    All tests passed!\n","    \n","\n","nltk_norm_tokens_punc:\n","\n","    All tests passed!\n","    \n","\n","nltk_whitespace_tokenize_and_nltk_normpunc_tokenize:\n","\n","    All tests passed!\n","    \n","\n","norm_tokens_punc:\n","\n","    All tests passed!\n","    \n","\n","normalize_token:\n","\n","    All tests passed!\n","    \n","\n","token_count:\n","\n","    All tests passed!\n","    \n","\n","token_count_punc:\n","\n","    All tests passed!\n","    \n","\n","token_count_whitespace:\n","\n","    All tests passed!\n","    \n","\n","tokens_whitespace:\n","\n","    All tests passed!\n","    \n","\n","type_count:\n","\n","    All tests passed!\n","    \n","\n","type_count_norm_punc:\n","\n","    All tests passed!\n","    \n"],"text/html":["<p><strong>anywhere_1hot:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>anywhere_1hot_reverse:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>cosine_distance:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>euclidean_distance:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>example_bow:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>example_sow:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>hamming_distance:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>jaccard_distance:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>nltk_norm_tokens_punc:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>nltk_whitespace_tokenize_and_nltk_normpunc_tokenize:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>norm_tokens_punc:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>normalize_token:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>token_count:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>token_count_punc:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>token_count_whitespace:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>tokens_whitespace:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>type_count:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>type_count_norm_punc:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n"]},"metadata":{},"execution_count":58}],"source":["grader.check_all()"],"id":"1ea49682"}],"metadata":{"celltoolbar":"Tags","colab":{"provenance":[]},"kernelspec":{"display_name":"otter-latest","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"title":"CS236299 Lab 1-1: Types, tokens, and representing text","vscode":{"interpreter":{"hash":"4fba83c08fc02185bb2310bd24d0cd81fb04529c933f82aa81c61aab9d5528dc"}}},"nbformat":4,"nbformat_minor":5}